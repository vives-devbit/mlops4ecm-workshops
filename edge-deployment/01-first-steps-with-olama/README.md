# üß™ Lab 01 ‚Äì First Steps with OLAMA

> Run your first LLM and vision models on a ROCK 5B embedded device. Then compare to GPU inference on a server. Discover the limits of local AI execution and get familiar with prompt-based interactions.

## üéØ Objectives

- Run a large language model (LLM) on an embedded device (ROCK 5B)
- Interact with a vision-capable LLM
- Benchmark performance: CPU vs GPU
- Explore model selection on the Ollama platform

## üñ•Ô∏è Setup on the ROCK 5B

### üîå Step 1 ‚Äì SSH into the Device

Each ROCK 5B is accessible on the network. Use the label on the device to find its IP:

```bash
ssh radxa@10.26.3.XX
# password: radxa
````

### ‚öôÔ∏è Step 2 ‚Äì Install Ollama

```bash
curl -fsSL https://ollama.com/install.sh | sh
```

This will:

* Create system groups and a user
* Enable the `ollama` systemd service
* Start the local API on port `11434`

The service starts automatically, so no need to run `ollama serve`.

### ü§ñ Step 3 ‚Äì Run Your First Model

Pull and run a small text model to test the setup:

```bash
ollama pull gemma3:1b
ollama run gemma3:1b
```

Try a basic prompt:

```
> What is an orange?
```

### üß™ Step 4 ‚Äì Benchmark with `--verbose`

```bash
ollama run gemma3:1b --verbose
```

You'll see detailed metrics:

* Total duration
* Prompt evaluation time
* Tokens per second

Example:

```
prompt eval rate:     20.62 tokens/s
eval rate:            13.67 tokens/s
```

### üì¶ Step 5 ‚Äì Try a Larger Model

Now try the 4B model:

```bash
ollama pull gemma3:4b
ollama run gemma3:4b --verbose
```

Observe the performance drop:

```
eval rate: 5.95 tokens/s (much slower)
```

## üñºÔ∏è Step 6 ‚Äì Run Vision Inference (Apple Image)

Download a sample image:

```bash
wget -O image.jpg https://upload.wikimedia.org/wikipedia/commons/1/15/Red_Apple.jpg
```

Then ask the model to describe it:

```bash
ollama run gemma3:4b --verbose
> What is in the ./image.jpg file?
```

This will take several **minutes** on the ROCK 5B (all CPU cores will max out). Time to try the server!

## üöÄ Setup on the GPU Server

### üîê Step 7 ‚Äì SSH into the Server

```bash
ssh root@10.26.28.XX
```

### ‚öôÔ∏è Step 8 ‚Äì Install Ollama (x86\_64 + GPU)

```bash
curl -fsSL https://ollama.com/install.sh | sh
```

Ollama will detect the NVIDIA GPU and enable GPU acceleration automatically.

### üñºÔ∏è Step 9 ‚Äì Vision Prompt on the Server

Download the same image again:

```bash
wget -O image.jpg https://upload.wikimedia.org/wikipedia/commons/1/15/Red_Apple.jpg
```

Run inference:

```bash
ollama run gemma3:4b --verbose
> What is in the ./image.jpg file?
```

This time it runs in **\~2.5 seconds**:

```
prompt eval rate: 152.60 tokens/s
eval rate: 71.95 tokens/s
```

## üîç Step 10 ‚Äì Explore Ollama Models

Visit [https://ollama.com](https://ollama.com)
Browse the models available ‚Äî look for:

* **Text-only** models (Mistral, Phi, LLaMA)
* **Vision** models (check if vision input is supported)
* Models with better CPU support or smaller size

You can try other models by pulling and running them:

```bash
ollama pull phi:1.5
ollama run phi:1.5
```

## üí° Step 11 - Experiments

* How big a model can you run on the ROCK 5B and server before it crashes?
* How many seconds did Gemma vision inference take on ROCK 5B vs GPU server?
* What‚Äôs the largest model you found on the Ollama website?

**Take your time** to really play with this. Get a feel for how "smart" or "stupid" some of these models are.
