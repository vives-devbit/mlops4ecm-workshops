# ğŸ§ª Lab 05 â€“ Explore Whisper Model through ONNX

In this lab, you'll explore the **Whisper model** from OpenAI â€” a transformer-based model for **automatic speech recognition (ASR)**, also known as **audio transcription**.

Youâ€™ll learn:
- What Whisper is and how it works
- How to run it using Hugging Face Transformers
- How to export parts of the model to ONNX format
- How to **visually explore the model** using Netron
- How **quantization** affects the model size and structure

## ğŸ§  What Is Whisper?

Whisper is a deep learning model that can **transcribe spoken language** into written text. You give it a short audio recording, and it tells you what was said.

Itâ€™s built using a **transformer architecture**, just like BERT or GPT â€” which youâ€™ve seen in earlier labs. Specifically, Whisper uses:

- An **encoder**, which processes the input audio (as a mel spectrogram)
- A **decoder**, which generates text token-by-token, conditioned on the encoder's output

<img src="../../media/openai-whisper-architecture.png" style="width: 100%">

## ğŸ¯ Goal of This Lab

Your goal in this lab is **not to train Whisper**, but to understand how it works by:
- Running it once with Hugging Face
- Exporting parts of the model to ONNX
- Opening those ONNX models in [Netron](https://netron.app/)
- Understanding what youâ€™re looking at: **encoders**, **decoders**, and what happens during **quantization**

## ğŸ› ï¸ Step 1 â€“ Connect to Your VM

As always, start by connecting to your assigned **virtual machine** using **Visual Studio Code** with remote SSH.

Then activate the Python environment and install the required packages:

```bash
source ~/mlops-workshops/.venv/bin/activate
cd ~/mlops-workshops/edge-deployment/05-explore-whisper
pip install -r requirements.txt
````

## ğŸ™ï¸ Step 2 â€“ Record Some Audio

Weâ€™ll begin by recording a short audio clip.

Run the Gradio app:

```bash
python 01-record-audio-app.py
```

Youâ€™ll see two URLs printed:

```
* Running on local URL:  http://0.0.0.0:7860
* Running on public URL: https://...gradio.live
```

âš ï¸ **Use the public URL** (HTTPS is required for microphone access in most browsers)

Open the URL in your browser and record yourself saying something **in English**. Click â€œSave Recordingâ€ and download the WAV file if you want to keep it. The file will also be saved as `speech.wav` in your VM.

## ğŸ§¾ Step 3 â€“ Transcribe with Hugging Face

Run the second script to transcribe your audio file:

```bash
python 02-whisper-huggingface.py
```

This uses Hugging Face Transformers to load the Whisper model and run inference. You'll see something like:

```
Transcription:

Hello, my name is Alexander and I'm a teacher at VIVES University of Applied Sciences.
```

Nice! ğŸ‰ You just did state-of-the-art transcription using a transformer model â€” directly on your VM.

ğŸ§  **Why Hugging Face?**
Itâ€™s easy to use, great for exploration and prototyping. But for edge deployment, we want something more lightweight â€” like ONNX.

## ğŸ“¦ Step 4 â€“ Export Encoder and Decoder to ONNX

Now letâ€™s export the Whisper **encoder** and **decoder** separately to ONNX format, so we can explore them visually.

Run:

```bash
python 03-export-encoder-decoder.py
```

Youâ€™ll see a lot of logs â€” including some ONNX runtime warnings. You can ignore those for now.

The script creates two files:

* `whisper_model_dir/whisper-base_encoder.onnx`
* `whisper_model_dir/whisper-base_decoder.onnx`

## ğŸ’» Step 5 â€“ Download and Explore in Netron

Open the **file explorer in VS Code**, right-click the ONNX files, and choose **Download**.

Visit [https://netron.app](https://netron.app) and **open the encoder model** first.

### ğŸ‘€ What to Look For

* The encoder starts with a **stack of transformer blocks** â€” each has self-attention and feedforward layers.
* At the end, youâ€™ll see `LayerNorm` and many parallel branches â€” these are used to prepare attention **keys** and **values** for the decoder (called cross-attention heads).

Now open the **decoder model**.

* The decoder starts with embeddings and input tokens
* It includes layers for both **self-attention** and **cross-attention**
* The structure is more complex, but familiar: attention + residuals + normalization

## ğŸ”¬ Step 6 â€“ Quantize the Model

Letâ€™s try exporting a smaller version of the model using quantization.

Run:

```bash
python 03-export-encoder-decoder.py --precision int8
```

Or for half-precision:

```bash
python 03-export-encoder-decoder.py --precision fp16
```

This will generate files like:

* `whisper_model_dir/whisper-base_encoder_int8.onnx`
* `whisper_model_dir/whisper-base_decoder_int8.onnx`

## ğŸ“‰ Step 7 â€“ Compare File Sizes

Before opening them in Netron, compare the file sizes:

* `fp32` (default): \~90â€“300 MB
* `fp16`: roughly **half the size**
* `int8`: roughly **one-quarter the size**

You can see this easily in your file browser.

## ğŸ§  Step 8 â€“ Visual Differences in Netron

Download and open the quantized models in Netron. Place your browser windows side-by-size with the standard fp32 models, to easily spot the differences.

### Whatâ€™s Different?

Youâ€™ll notice:

* New layers like `DynamicQuantizeLinear`
* Many `MatMul` nodes replaced with `MatMulInteger`
* Possibly some reordering or structural simplification

This shows you how ONNX applies **quantization-aware operations** to reduce memory and speed up inference â€” especially useful on edge devices.

## âœ… Wrap-Up

You just:

* Explored the Whisper model in code and in structure
* Ran real audio transcription
* Exported the model for inspection
* Compared different model precisions and their visual graphs

In the next lab, youâ€™ll take this further â€” combining preprocessing, inference, and decoding into a **single ONNX model** and deploying it on an embedded device.
