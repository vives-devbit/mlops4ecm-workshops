{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c475f760",
   "metadata": {},
   "source": [
    "### ü¶ñ Grounding DINO ‚Äì Object Detection Through Language\n",
    "\n",
    "Grounding DINO is an **open-vocabulary object detection model** that can locate and label objects in an image using free-form natural language. Unlike traditional object detectors trained on fixed label sets, Grounding DINO allows you to specify **any objects you want to detect** by simply typing them into a text box.\n",
    "\n",
    "This notebook launches a **Gradio app** where you can upload an image and enter a prompt describing the objects you're interested in ‚Äî for example:  \n",
    "```\n",
    "cat. dog. person.\n",
    "```\n",
    "\n",
    "üëâ **Important:** Separate your target labels with **periods (`.`)**.  \n",
    "The model will use this prompt to find and draw boxes around the matching objects in your image.\n",
    "\n",
    "Try uploading your own photos or test images, and experiment with both simple and unusual prompts. You can detect everyday objects ‚Äî or test how the model handles rare or unexpected terms!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c48bd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image, ImageDraw\n",
    "import gradio as gr\n",
    "from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection\n",
    "\n",
    "# Load model & processor\n",
    "model_id = \"IDEA-Research/grounding-dino-base\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "model = AutoModelForZeroShotObjectDetection.from_pretrained(model_id).to(device)\n",
    "\n",
    "def detect_and_draw(image, prompt):\n",
    "    if not prompt.endswith(\".\"):\n",
    "        prompt += \".\"  # Add dot if missing\n",
    "\n",
    "    inputs = processor(images=image, text=prompt.lower(), return_tensors=\"pt\").to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    results = processor.post_process_grounded_object_detection(\n",
    "        outputs,\n",
    "        inputs.input_ids,\n",
    "        box_threshold=0.4,\n",
    "        text_threshold=0.3,\n",
    "        target_sizes=[image.size[::-1]]\n",
    "    )[0]\n",
    "\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    for box, label, score in zip(results[\"boxes\"], results[\"labels\"], results[\"scores\"]):\n",
    "        box = box.tolist()\n",
    "        draw.rectangle(box, outline=\"red\", width=3)\n",
    "        draw.text((box[0], box[1]), f\"{label} ({score:.2f})\", fill=\"white\")\n",
    "\n",
    "    return image\n",
    "\n",
    "# Add explanation under prompt input\n",
    "label_prompt = gr.Textbox(\n",
    "    label=\"Text Prompt\",\n",
    "    placeholder=\"e.g., a cat. a remote control.\",\n",
    "    info=\"‚ö†Ô∏è Use lowercase labels and end each with a period (e.g., 'a cat. a remote control.'). This is required for Grounding DINO.\"\n",
    ")\n",
    "\n",
    "# Launch the interface\n",
    "gr.Interface(\n",
    "    fn=detect_and_draw,\n",
    "    inputs=[\n",
    "        gr.Image(type=\"pil\", label=\"Upload an Image\"),\n",
    "        label_prompt\n",
    "    ],\n",
    "    outputs=gr.Image(type=\"pil\", label=\"Detected Image\"),\n",
    "    title=\"Grounding DINO Object Detector\",\n",
    "    description=\"Detect objects using natural language prompts. Separate object names with periods and use lowercase.\"\n",
    ").launch(server_name=\"0.0.0.0\", server_port=8080, share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
