{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5951cafc",
   "metadata": {},
   "source": [
    "## üñºÔ∏è Lab 5 ‚Äì Story with Images\n",
    "\n",
    "In this notebook, you're going to combine everything you've built so far and bring your story to life with illustrations.\n",
    "\n",
    "You'll use a **diffusion model** to generate one image for each part of your story, based on the text of that chapter. By the end, you'll have a complete, multi-part story ‚Äî with both **text and AI-generated images** ‚Äî displayed together in the notebook.\n",
    "\n",
    "This is where your project becomes truly multimodal!\n",
    "\n",
    "### What We'll Do:\n",
    "\n",
    "- Generate a table of contents from your story idea (like in Lab 4)\n",
    "- Use the LLM to write each chapter, one at a time\n",
    "- For each chapter, generate a matching image using a diffusion model\n",
    "- Display the full story with illustrations\n",
    "\n",
    "Let‚Äôs bring your story to life!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4a4afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì¶ Load the diffusion model (run this cell only once)\n",
    "from diffusers import StableDiffusion3Pipeline\n",
    "import torch\n",
    "\n",
    "pipe = StableDiffusion3Pipeline.from_pretrained(\n",
    "    \"ckpt/stable-diffusion-3.5-medium\", \n",
    "    torch_dtype=torch.bfloat16\n",
    ").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d925085b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "from PIL import Image\n",
    "import ollama\n",
    "\n",
    "# üîß Configuration\n",
    "MODEL_NAME = \"gemma3:4b-it-qat\"\n",
    "NUM_PARTS = 5\n",
    "\n",
    "# ‚úçÔ∏è TODO: Fill in your story idea below\n",
    "story_idea = \"\"\n",
    "\n",
    "assert story_idea != \"\"\n",
    "\n",
    "# STEP 1: Generate table of contents\n",
    "system_prompt_toc = f\"\"\"\n",
    "You are a storyteller. We're going to generate a multi-part story.\n",
    "Your task is to plan the structure, by generating story abstracts.\n",
    "\n",
    "Given the user prompt as idea, generate {NUM_PARTS} one-line abstracts.\n",
    "Each abstract should represent the content of one part of the story.\n",
    "Do not write the full story yet. Just output the abstracts, one per line.\n",
    "Only output the {NUM_PARTS} one-line abstracts. Nothing else.\n",
    "\"\"\"\n",
    "\n",
    "response_toc = ollama.chat(\n",
    "    model=MODEL_NAME,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt_toc},\n",
    "        {\"role\": \"user\", \"content\": story_idea}\n",
    "    ]\n",
    ")\n",
    "\n",
    "abstracts_raw = response_toc[\"message\"][\"content\"].strip()\n",
    "abstracts = [line.strip() for line in abstracts_raw.split(\"\\n\") if line.strip()]\n",
    "abstracts_full = \"\\n\".join(abstracts)\n",
    "\n",
    "print(\"=== Abstracts ===\")\n",
    "print(abstracts_full)\n",
    "\n",
    "# STEP 2: Generate story chapters + illustrations\n",
    "full_story_so_far = \"\"\n",
    "\n",
    "for i, abstract in enumerate(abstracts):\n",
    "    # Text generation\n",
    "    system_prompt_chapter = \"\"\"\n",
    "You are a storyteller. We're building a multi-part story.\n",
    "Please write the next part of the story.\n",
    "Use previous parts as context to ensure continuity.\n",
    "Make sure the text flows naturally from earlier events.\n",
    "\"\"\"\n",
    "\n",
    "    user_prompt_chapter = f\"\"\"\n",
    "All abstracts for the full story (one per line):\n",
    "{abstracts_full}\n",
    "\n",
    "Here is the story so far:\n",
    "{full_story_so_far}\n",
    "\n",
    "Now continue with part {i+1}: {abstract}\n",
    "Each part should be no more than 2 paragraphs!\n",
    "\"\"\"\n",
    "\n",
    "    response_chapter = ollama.chat(\n",
    "        model=MODEL_NAME,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt_chapter},\n",
    "            {\"role\": \"user\", \"content\": user_prompt_chapter}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    chapter_text = response_chapter[\"message\"][\"content\"].strip()\n",
    "    full_story_so_far += \"\\n\\n\" + chapter_text\n",
    "\n",
    "    # Display text\n",
    "    display(Markdown(f\"### Part {i+1}: {abstract}\"))\n",
    "    display(Markdown(chapter_text))\n",
    "\n",
    "    # üß† TODO: Turn the chapter text into an image prompt for the diffusion model\n",
    "    # \n",
    "    # Right now, 'image_prompt' is empty, so the cell will crash until you fix it.\n",
    "    # Your task: extract or create a short prompt that describes the scene.\n",
    "    #\n",
    "    # ‚ö†Ô∏è The diffusion model only accepts prompts up to 77 characters ‚Äî longer text will be cut off!\n",
    "    #\n",
    "    # üí° Tips:\n",
    "    # - The simplest version is to just use the beginning of the chapter:\n",
    "    #     image_prompt = chapter_text[:77]\n",
    "    # - You can extract the first sentence using .split(\".\")[0]\n",
    "    # - A better solution: use the LLM to generate a visual description\n",
    "    #     (e.g., \"Describe this scene in one line for a text-to-image model.\")\n",
    "    # - Try calling your LLM again inside the loop to do this!\n",
    "\n",
    "    image_prompt = \"\"  # üëà TODO: Replace this with your own logic!\n",
    "\n",
    "    assert image_prompt != \"\"\n",
    "\n",
    "    # Generate and display image\n",
    "    image = pipe(\n",
    "        prompt=image_prompt,\n",
    "        num_inference_steps=20,\n",
    "        guidance_scale=5,\n",
    "        width=512,\n",
    "        height=512\n",
    "    ).images[0]\n",
    "\n",
    "    display(image)\n",
    "\n",
    "    # Optional: Save to file\n",
    "    # image.save(f\"chapter-{i+1}.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
