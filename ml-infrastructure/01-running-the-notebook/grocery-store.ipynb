{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "619b5703",
   "metadata": {},
   "source": [
    "# üõí Grocery Store ‚Äì Training an Image Classifier\n",
    "\n",
    "In this notebook, we‚Äôll build and train a simple image classification model to recognize grocery items from photos. The dataset contains labeled images of various food products, each assigned to a coarse category like \"vegetables\", \"dairy\", or \"snacks\".\n",
    "\n",
    "We‚Äôll walk through a full training workflow, including:\n",
    "\n",
    "- Loading and preprocessing image data\n",
    "- Defining a PyTorch dataset and data loaders\n",
    "- Fine-tuning a pretrained MobileNet model\n",
    "- Evaluating the model's performance on a validation set\n",
    "- Visualizing sample predictions with images\n",
    "\n",
    "By the end, you'll have a basic image classifier running locally, ready to be served or deployed in later steps of the workshop.\n",
    "\n",
    "**üìö Importing Packages**\n",
    "\n",
    "This section imports all necessary Python libraries:\n",
    "\n",
    "- `os`, `pandas`, `matplotlib.pyplot`: for file handling, tabular data, and visualization\n",
    "- `torch`, `torchvision`, `torch.nn`, `torch.optim`: core PyTorch libraries for deep learning\n",
    "- `PIL.Image`: for loading and manipulating image files\n",
    "- `tqdm`: to show progress bars during training loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cf164d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.models import mobilenet_v3_small, MobileNet_V3_Small_Weights\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b0066a",
   "metadata": {},
   "source": [
    "**‚öôÔ∏è Configuration and Class Labels**\n",
    "\n",
    "This section defines basic training parameters and paths to the dataset files.\n",
    "\n",
    "- Sets the **data directory** and **file locations** for class labels, training set, and validation set.\n",
    "- Defines training **hyperparameters** like batch size, learning rate, and number of epochs.\n",
    "- Detects whether a **GPU is available** and selects the appropriate device (CPU or CUDA).\n",
    "- Loads the mapping from numeric **class IDs** to human-readable names from `classes.csv`.\n",
    "\n",
    "**NOTE**: This notebook relies on `~/GroceryStoreDataset` already being present in the home folder.\n",
    "\n",
    "If it is not, simply run: `git clone https://github.com/marcusklasson/GroceryStoreDataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5250b022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "DATA_DIR = os.path.expanduser('~/GroceryStoreDataset/dataset')\n",
    "CLASSES_CSV = os.path.join(DATA_DIR, 'classes.csv')\n",
    "TRAIN_CSV = os.path.join(DATA_DIR, 'train.txt')\n",
    "VAL_CSV = os.path.join(DATA_DIR, 'val.txt')\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 5\n",
    "LEARNING_RATE = 1e-3\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load class names\n",
    "CLASSES = (  # dictionary int => class name\n",
    "    pd.read_csv(CLASSES_CSV)\n",
    "    .drop_duplicates(\"Coarse Class ID (int)\")\n",
    "    .set_index(\"Coarse Class ID (int)\")[\"Coarse Class Name (str)\"]\n",
    "    .to_dict()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf6c556",
   "metadata": {},
   "source": [
    "**üß∫ Dataset, Data Loaders, and Model Setup**\n",
    "\n",
    "This section prepares the **dataset** and sets up the **model**. We define a custom `GroceryDataset` class that loads images and labels from CSV files provided in the Grocery Store Dataset. Each image has both a fine-grained label (specific product) and a coarse-grained label (product category). In this notebook, we focus on the **coarse labels** for simplicity.\n",
    "\n",
    "To prepare the images, we apply a basic **preprocessing** pipeline: **resizing** each image to 224√ó224 pixels, converting it to a PyTorch tensor, and **normalizing** the pixel values using the standard mean and standard deviation from ImageNet. This normalization ensures the input distribution matches what the MobileNet model expects.\n",
    "\n",
    "We use PyTorch `DataLoader`s to handle batching and shuffling of the training and validation data. For the model, we load a pre-trained **MobileNetV3 Small** architecture ‚Äî a compact and efficient network well-suited for edge deployment. We replace the final **classification layer** to match the number of classes in our dataset and move the model to the appropriate device (CPU or GPU).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87f74d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class\n",
    "class GroceryDataset(Dataset):\n",
    "    def __init__(self, csv_file, transform=None):\n",
    "        self.df = pd.read_csv(csv_file, header=None)\n",
    "        self.df.columns = [\"path\", \"fine_label\", \"coarse_label\"]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_path = os.path.join(DATA_DIR, row[\"path\"])\n",
    "        label = row[\"coarse_label\"]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# Image transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Data loaders\n",
    "train_dataset = GroceryDataset(TRAIN_CSV, transform=transform)\n",
    "val_dataset = GroceryDataset(VAL_CSV, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Model\n",
    "model = mobilenet_v3_small(weights=MobileNet_V3_Small_Weights.DEFAULT)\n",
    "model.classifier[3] = nn.Linear(model.classifier[3].in_features, len(CLASSES))\n",
    "model = model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba66e5cc",
   "metadata": {},
   "source": [
    "**üèãÔ∏è Training the Model**\n",
    "\n",
    "We now set up the **training loop**. The model is trained using the **Adam optimizer** and **cross-entropy loss**, which is suitable for multi-class classification tasks.\n",
    "\n",
    "For each epoch, we iterate over the training dataset in batches. We move data to the correct device, perform a **forward pass** through the model, **compute the loss**, **backpropagates** the gradients, and **update the model** weights. The average training loss per epoch is printed to track progress over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ad5172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training setup\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
    "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * images.size(0)\n",
    "    print(f\"Epoch {epoch+1} - Loss: {total_loss / len(train_loader.dataset):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2bfa0c6",
   "metadata": {},
   "source": [
    "**üìä Model Evaluation**\n",
    "\n",
    "After training, we evaluate the model on the **validation set** to check **how well it generalizes**. We switch the model to **evaluation mode** and disable gradient tracking for efficiency. For each batch, we compute predictions and compare them to the ground truth labels. At the end, we print the overall **validation accuracy** as a percentage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad635ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in val_loader:\n",
    "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "print(f\"\\nValidation Accuracy: {correct / total:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8b97f9",
   "metadata": {},
   "source": [
    "**üñºÔ∏è Visualizing Predictions**\n",
    "\n",
    "To better understand the model's behavior, we display **10 sample predictions** from the validation set. For each image, we show the predicted and true class names. The pixel values are unnormalized for display so the images look natural. This kind of qualitative inspection is useful for spotting obvious misclassifications or biases in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca81d0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show predictions with images\n",
    "print(\"\\nSample predictions with images:\")\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Get 10 images, predictions, and labels again for visualization\n",
    "shown = 0\n",
    "for images, labels in val_loader:\n",
    "    images_cpu = images.cpu()\n",
    "    outputs = model(images.to(DEVICE))\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    predicted = predicted.cpu()\n",
    "\n",
    "    for i in range(images.size(0)):\n",
    "        if shown >= 10:\n",
    "            break\n",
    "        img = images_cpu[i].permute(1, 2, 0).numpy()\n",
    "        img = img * [0.229, 0.224, 0.225] + [0.485, 0.456, 0.406]  # unnormalize\n",
    "        img = img.clip(0, 1)\n",
    "\n",
    "        axes[shown].imshow(img)\n",
    "        axes[shown].axis(\"off\")\n",
    "        axes[shown].set_title(f\"Predict: {CLASSES[predicted[i].item()]}\\nTrue label: {CLASSES[labels[i].item()]}\")\n",
    "        shown += 1\n",
    "\n",
    "    if shown >= 10:\n",
    "        break\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
